{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trigger Word Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import  libraries\n",
    "import numpy as np\n",
    "from pydub import AudioSegment\n",
    "import random\n",
    "import sys\n",
    "import io\n",
    "import os\n",
    "import glob\n",
    "import IPython\n",
    "from td_utils import *\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use positives/negatives/backgrounds recordings to create a labeled dataset.\n",
    "Tx = 5511\n",
    "n_freq = 101\n",
    "Ty = 1375\n",
    "\n",
    "#load audio segments using pydub \n",
    "activates, negatives, backgrounds = load_raw_audio()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_time_segment(segment_ms):\n",
    "    \n",
    "    segment_start = np.random.randint(low=0, high=10000-segment_ms)\n",
    "    segment_end = segment_start + segment_ms - 1\n",
    "    \n",
    "    return (segment_start, segment_end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_overlapping(segment_time, previous_segments):\n",
    "    \n",
    "    segment_start, segment_end = segment_time\n",
    "    overlap = False\n",
    "\n",
    "    for previous_start, previous_end in previous_segments:\n",
    "        if segment_start <= previous_end and segment_end >= previous_start:\n",
    "            overlap = True\n",
    "\n",
    "    return overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_audio_clip(background, audio_clip, previous_segments):\n",
    "\n",
    "    segment_ms = len(audio_clip)\n",
    "    segment_time = get_random_time_segment(segment_ms)\n",
    "\n",
    "    while is_overlapping(segment_time, previous_segments):\n",
    "        segment_time = get_random_time_segment(segment_ms)\n",
    "\n",
    "    previous_segments.append(segment_time)\n",
    "\n",
    "    new_background = background.overlay(audio_clip, position = segment_time[0])\n",
    "    \n",
    "    return new_background, segment_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_ones(y, segment_end_ms):\n",
    "    \n",
    "    segment_end_y = int(segment_end_ms*Ty/10000.0)\n",
    "    \n",
    "    for i in range(segment_end_y+1, segment_end_y+51):\n",
    "        if i < Ty:\n",
    "            y[0,i] = 1\n",
    "    \n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create training examples\n",
    "\n",
    "def create_training_example(background, activates, negatives):\n",
    "   \n",
    "    np.random.seed(18)\n",
    "    \n",
    "    background = background - 20\n",
    "    \n",
    "    y = np.zeros((1, Ty))\n",
    "    \n",
    "    previous_segments = []\n",
    "\n",
    "    number_of_activates = np.random.randint(0, 5)\n",
    "    random_indices = np.random.randint(len(activates), size=number_of_activates)\n",
    "    random_activates = [activates[i] for i in random_indices]\n",
    "    \n",
    "    for random_activate in random_activates:\n",
    "        background, segment_time = insert_audio_clip(background, random_activate, previous_segments)\n",
    "        segment_start, segment_end = segment_time\n",
    "        y = insert_ones(y, segment_end_ms=segment_end)\n",
    "\n",
    "    number_of_negatives = np.random.randint(0, 3)\n",
    "    random_indices = np.random.randint(len(negatives), size=number_of_negatives)\n",
    "    random_negatives = [negatives[i] for i in random_indices]\n",
    "\n",
    "    for random_negative in random_negatives:\n",
    "        background, _ = insert_audio_clip(background, random_negative, previous_segments)\n",
    "    \n",
    "    background = match_target_amplitude(background, -20.0)\n",
    "\n",
    "    file_handle = background.export(\"train\" + \".wav\", format=\"wav\")\n",
    "    print(\"File (train.wav) was saved in your directory.\")\n",
    "    \n",
    "    x = graph_spectrogram(\"train.wav\")\n",
    "    \n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load preprocessed training examples\n",
    "X = np.load(\"./XY_train/X.npy\")\n",
    "Y = np.load(\"./XY_train/Y.npy\")\n",
    "\n",
    "#load preprocessed dev set examples\n",
    "X_dev = np.load(\"./XY_dev/X_dev.npy\")\n",
    "Y_dev = np.load(\"./XY_dev/Y_dev.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load libraries for the model\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.models import Model, load_model, Sequential\n",
    "from keras.layers import Dense, Activation, Dropout, Input, Masking, TimeDistributed, LSTM, Conv1D\n",
    "from keras.layers import GRU, Bidirectional, BatchNormalization, Reshape\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#build the model\n",
    "\n",
    "def model(input_shape):\n",
    "    \n",
    "    X_input = Input(shape = input_shape)\n",
    "    \n",
    "    #step 1: CONV layer\n",
    "    X = Conv1D(196, kernel_size = 15, strides = 4)(X_input)\n",
    "    X = BatchNormalization()(X)                                \n",
    "    X = Activation('relu')(X)                  \n",
    "    X = Dropout(0.8)(X)                \n",
    "\n",
    "    #step 2: First GRU Layer\n",
    "    X = GRU(units = 128, return_sequences = True)(X)                            \n",
    "    X = Dropout(0.8)(X)                             \n",
    "    X = BatchNormalization()(X)                              \n",
    "    \n",
    "    #step 3: Second GRU Layer\n",
    "    X = GRU(units = 128, return_sequences = True)(X)                           \n",
    "    X = Dropout(0.8)(X)                                \n",
    "    X = BatchNormalization()(X)                                 \n",
    "    X = Dropout(0.8)(X)                         \n",
    "    \n",
    "    #step 4: Time-distributed dense layer\n",
    "    X = None\n",
    "\n",
    "    model = Model(inputs = X_input, outputs = X)\n",
    "    \n",
    "    return model  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#implement the model\n",
    "model = model(input_shape = (Tx, n_freq))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the pre-trained model\n",
    "model = load_model('./models/tr_model.h5')\n",
    "\n",
    "#fit the model\n",
    "opt = Adam(lr=0.0001, beta_1=0.9, beta_2=0.999, decay=0.01)\n",
    "model.compile(loss='binary_crossentropy', optimizer=opt, metrics=[\"accuracy\"])\n",
    "model.fit(X, Y, batch_size = 5, epochs=1)\n",
    "\n",
    "#test the model\n",
    "loss, acc = model.evaluate(X_dev, Y_dev)\n",
    "print(\"Dev set accuracy = \", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#determine the predict function\n",
    "def detect_triggerword(filename):\n",
    "    plt.subplot(2, 1, 1)\n",
    "\n",
    "    x = graph_spectrogram(filename)\n",
    "    x  = x.swapaxes(0,1)\n",
    "    x = np.expand_dims(x, axis=0)\n",
    "    predictions = model.predict(x)\n",
    "    \n",
    "    plt.subplot(2, 1, 2)\n",
    "    plt.plot(predictions[0,:,0])\n",
    "    plt.ylabel('probability')\n",
    "    plt.show()\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#insert a chime to acknowledge the \"activate\" trigger\n",
    "chime_file = \"audio_examples/chime.wav\"\n",
    "def chime_on_activate(filename, predictions, threshold):\n",
    "    audio_clip = AudioSegment.from_wav(filename)\n",
    "    chime = AudioSegment.from_wav(chime_file)\n",
    "    Ty = predictions.shape[1]\n",
    "    #step 1: Initialize the number of consecutive output steps to 0\n",
    "    consecutive_timesteps = 0\n",
    "    #step 2: Loop over the output steps in the y\n",
    "    for i in range(Ty):\n",
    "        #step 3: Increment consecutive output steps\n",
    "        consecutive_timesteps += 1\n",
    "        #step 4: If prediction is higher than the threshold and more than 75 consecutive output steps have passed\n",
    "        if predictions[0,i,0] > threshold and consecutive_timesteps > 75:\n",
    "            #step 5: Superpose audio and background using pydub\n",
    "            audio_clip = audio_clip.overlay(chime, position = ((i / Ty) * audio_clip.duration_seconds)*1000)\n",
    "            #step 6: Reset consecutive output steps to 0\n",
    "            consecutive_timesteps = 0\n",
    "        \n",
    "    audio_clip.export(\"chime_output.wav\", format='wav')"
   ]
  }
 ],
 "metadata": {
  "coursera": {
   "course_slug": "nlp-sequence-models",
   "graded_item_id": "rSupZ",
   "launcher_item_id": "cvGhe"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
